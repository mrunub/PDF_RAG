{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    # for page_num in range(len(pdf_reader.pages)):\n",
    "    for page_num in range(10):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = r'C:\\Users\\HP\\OneDrive\\14Code\\GPT\\docs\\app\\ConceptsofBiology-WEB.pdf'\n",
    "document_text = extract_text_from_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(document_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path_book = r'C:\\Users\\HP\\OneDrive\\14Code\\GPT\\docs\\app\\ConceptsofBiology-WEB.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# # Load OpenAI API key from environment variable\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# if not openai_api_key:\n",
    "#     raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    # for page_num in range(len(pdf_reader.pages)):\n",
    "    for page_num in range(10):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Extract and chunk text\n",
    "pdf_path = pdf_path_book\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "chunks = chunk_text(document_text)\n",
    "\n",
    "# Embed the chunks using Sentence-Transformers\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Create a FAISS index\n",
    "d = embeddings.shape[1]  # dimension\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Wrap the FAISS index using LangChain's FAISS vector store\n",
    "vector_store = FAISS.from_embeddings(chunks, embeddings)\n",
    "\n",
    "# Set up LangChain with OpenAI GPT-4\n",
    "llm = OpenAI(model_name=OPENAI_DEPLOYMENT, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create a LangChain QA chain\n",
    "qa_chain = VectorDBQA(llm=llm, vectorstore=vector_store)\n",
    "\n",
    "# Function to answer queries\n",
    "def answer_query(query):\n",
    "    response = qa_chain({\"query\": query})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "# Example query\n",
    "query = \"What is the main topic of the document?\"\n",
    "answer = answer_query(query)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import pinecone\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(pdf_path_book)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()\n",
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[40].page_content[400:1000]}')\n",
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)\n",
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "# # load it into Chroma\n",
    "# vectorstore = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import pinecone\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "# # load it into Chroma\n",
    "# vectorstore = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrunmayee.babhulkar\\.conda\\envs\\autogen2\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts, \n",
    "    embedding=embedder\n",
    "    # persist_directory=CHROMA_PATH\n",
    ")\n",
    "# db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"how conflicting motivators function for a one-time behavior?\"\"\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of the first document that was returned\n",
    "for doc in docs:\n",
    "    print (f\"{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how conflicting motivators function for a one-time behavior?\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "vectorstore=FAISS.from_documents(texts,embedding=embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever=vectorstore.as_retriever() \n",
    "retriever.search_kwargs[\"distance_metric\"]='cos'\n",
    "retriever.search_kwargs[\"fetch_k\"]=100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"]=True\n",
    "retriever.search_kwargs[\"k\"]=20\n",
    "\n",
    "model =AzureChatOpenAI(azure_deployment=\"\",\n",
    "    openai_api_version=\"\",\n",
    "    model=\"\",\n",
    "    openai_api_key=\"\",\n",
    "    azure_endpoint=\"\")\n",
    "qa=ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'how conflicting motivators function for a one-time behavior?', 'chat_history': [], 'answer': 'Conflicting motivators can drive a person towards different behaviors even for a one-time behavior. For example, at a company party, a person may have conflicting motivations to take a rest and enjoy their newly scrubbed house, but they may also want to tackle the backyard and cross that project off their list. These competing motivations can push a person towards different behaviors.'}\n"
     ]
    }
   ],
   "source": [
    "question=\"how conflicting motivators function for a one-time behavior?\"\n",
    "chat_history=[]\n",
    "result=qa({\"question\":question,\"chat_history\":chat_history})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
